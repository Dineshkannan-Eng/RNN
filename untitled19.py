# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-rWf7NMmiJiSYlit9xIm2Lu-h9iMRhcj
"""

# ==========================================================
# Advanced Time Series Forecasting with Attention Mechanism
# ==========================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.layers import (
    LSTM, GRU, Dense, Dropout, Input, Layer
)
from tensorflow.keras.models import Model
import keras_tuner as kt

# ----------------------------------------------------------
# 1. Generate Synthetic Multivariate Dataset (N=5000)
# ----------------------------------------------------------

def generate_dataset(n=5000):
    t = np.arange(n)

    trend = 0.0005 * t
    seasonal_daily = 0.5 * np.sin(2 * np.pi * t / 24)
    seasonal_weekly = 0.8 * np.sin(2 * np.pi * t / 168)
    noise = 0.2 * np.random.randn(n)

    x1 = np.sin(0.015 * t) + 0.1 * np.random.randn(n)
    x2 = np.cos(0.01 * t + 1) + 0.1 * np.random.randn(n)

    y = 1.2 * trend + 0.9 * seasonal_daily + 0.7 * seasonal_weekly + 0.5 * x1 + 0.3 * x2 + noise

    df = pd.DataFrame({"y": y, "x1": x1, "x2": x2})
    return df


df = generate_dataset()


# ----------------------------------------------------------
# 2. Sequence Generator (Lookback Window)
# ----------------------------------------------------------

def create_sequences(data, lookback=60):
    X, y = [], []
    for i in range(lookback, len(data)):
        X.append(data[i-lookback:i])
        y.append(data[i, 0])
    return np.array(X), np.array(y)


# ----------------------------------------------------------
# 3. Scaling and Window Prep
# ----------------------------------------------------------

scaler = MinMaxScaler()
scaled = scaler.fit_transform(df)

lookback = 60
X, y = create_sequences(scaled, lookback)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, shuffle=False, test_size=0.2
)

# ----------------------------------------------------------
# 4. Custom Attention Layer
# ----------------------------------------------------------

class Attention(Layer):
    def __init__(self):
        super(Attention, self).__init__()

    def build(self, input_shape):
        self.W = self.add_weight(name="att_weight",
                                 shape=(input_shape[-1], 1),
                                 initializer="normal",
                                 trainable=True)
        self.b = self.add_weight(name="att_bias",
                                 shape=(input_shape[1], 1),
                                 initializer="zeros",
                                 trainable=True)

    def call(self, x):
        e = tf.matmul(x, self.W) + self.b
        e = tf.squeeze(e, axis=-1)
        alpha = tf.nn.softmax(e)
        alpha = tf.expand_dims(alpha, axis=-1)
        context = x * alpha
        return tf.reduce_sum(context, axis=1), alpha


# ----------------------------------------------------------
# 5. Build Attention-LSTM Model (for Keras Tuner)
# ----------------------------------------------------------

def build_model(hp):

    units = hp.Int("units", min_value=32, max_value=128, step=32)
    dropout = hp.Float("dropout", 0.1, 0.5, step=0.1)
    lr = hp.Choice("lr", [1e-2, 1e-3, 1e-4])
    att_dim = hp.Int("att_dim", 32, 128, step=32)

    inputs = Input(shape=(lookback, X_train.shape[2]))
    lstm_out = LSTM(units, return_sequences=True)(inputs)
    lstm_out = Dropout(dropout)(lstm_out)

    att_layer = Attention()
    context_vector, attention_weights = att_layer(lstm_out)

    outputs = Dense(1)(context_vector)

    model = Model(inputs, outputs)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),
        loss="mse"
    )

    return model


# ----------------------------------------------------------
# 6. Hyperparameter Tuning
# ----------------------------------------------------------

tuner = kt.RandomSearch(
    build_model,
    objective="val_loss",
    max_trials=5,
    directory="attention_tuning",
    project_name="att_ts_project"
)

tuner.search(X_train, y_train, validation_split=0.2, epochs=10, verbose=1)
best_model = tuner.get_best_models(1)[0]


# ----------------------------------------------------------
# 7. Train Best Attention Model
# ----------------------------------------------------------

history = best_model.fit(
    X_train, y_train,
    epochs=20,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)

# ----------------------------------------------------------
# 8. Baseline Model (Plain LSTM)
# ----------------------------------------------------------

def build_baseline():
    model = tf.keras.Sequential([
        LSTM(64, input_shape=(lookback, X_train.shape[2])),
        Dense(1)
    ])
    model.compile(optimizer="adam", loss="mse")
    return model


baseline = build_baseline()
baseline.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)

# ----------------------------------------------------------
# 9. Evaluation
# ----------------------------------------------------------

def evaluate(model, X_test, y_test):
    pred = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, pred))
    mae = mean_absolute_error(y_test, pred)
    mape = np.mean(np.abs((y_test - pred) / y_test)) * 100
    return rmse, mae, mape


att_rmse, att_mae, att_mape = evaluate(best_model, X_test, y_test)
base_rmse, base_mae, base_mape = evaluate(baseline, X_test, y_test)

print("\n--- Baseline LSTM ---")
print("RMSE:", base_rmse)
print("MAE :", base_mae)
print("MAPE:", base_mape)

print("\n--- Attention-LSTM ---")
print("RMSE:", att_rmse)
print("MAE :", att_mae)
print("MAPE:", att_mape)


# ----------------------------------------------------------
# 10. Extract & Visualize Attention Weights
# ----------------------------------------------------------

attention_layer = best_model.layers[2]
attention_model = Model(best_model.input, attention_layer.output[1])

att_weights = attention_model.predict(X_test[:1])

plt.figure(figsize=(12, 4))
plt.plot(att_weights[0].squeeze())
plt.title("Attention Weights for First Test Sample")
plt.xlabel("Time Step (Lookback window)")
plt.ylabel("Attention Weight")
plt.show()